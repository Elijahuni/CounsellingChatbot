from typing import List, Dict
from data_processor import DataProcessor
from openai import OpenAI

class RAGEngine:
    def __init__(self, data_processor: DataProcessor, openai_api_key: str):
        self.data_processor = data_processor
        self.client = OpenAI(api_key=openai_api_key)
        
    def generate_context(self, query: str) -> str:
        """ìœ ì‚¬ ìƒë‹´ ì‚¬ë¡€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        similar_cases = self.data_processor.find_similar_cases(query, k=2)  # ìœ ì‚¬ ì‚¬ë¡€ ìˆ˜ ì¡°ì •
        
        context = "ë‹¤ìŒì€ ìœ ì‚¬í•œ ìƒë‹´ ì‚¬ë¡€ë“¤ì…ë‹ˆë‹¤:\n\n"
        for i, case in enumerate(similar_cases, 1):
            context += f"ì‚¬ë¡€ {i}:\n"
            if case.get('type') == 'multi':  # ë©€í‹°í„´ ëŒ€í™”ì¸ ê²½ìš°
                context += f"ëŒ€í™” ë‚´ìš©:\n{case['input']}\nìƒë‹´ì‚¬ ë‹µë³€:\n{case['output']}\n\n"
            else:  # ì‹±ê¸€í„´ ëŒ€í™”ì¸ ê²½ìš°
                context += f"ë‚´ë‹´ì: {case['input']}\nìƒë‹´ì‚¬: {case['output']}\n\n"
            
        return context
    
    def get_response(self, query: str, chat_history: List[Dict]) -> str:
        """RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""
        try:
            # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = self.generate_context(query)
            
            # íˆìŠ¤í† ë¦¬ ê¸¸ì´ë¡œ ëŒ€í™” ë‹¨ê³„ íŒŒì•…
            is_initial = len(chat_history) <= 2
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            system_prompt = """ë‹¹ì‹ ì€ 10ë…„ ì´ìƒì˜ ê²½ë ¥ì„ ê°€ì§„ ì „ë¬¸ ì‹¬ë¦¬ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. 
            ë‚´ë‹´ìì˜ ì´ì•¼ê¸°ì— ê¹Šì´ ìˆê²Œ ê·€ ê¸°ìš¸ì´ê³ , ê·¸ë“¤ì˜ ê°ì •ì„ ì„¬ì„¸í•˜ê²Œ ì´í•´í•˜ë©°, 
            ì ì ˆí•œ ì‹œì ì— í†µì°°ë ¥ ìˆëŠ” ì§ˆë¬¸ê³¼ ë°˜ì˜ì„ ì œê³µí•©ë‹ˆë‹¤.

ì „ë°˜ì ì¸ ìƒë‹´ ì›ì¹™:
1. ê²½ì²­ê³¼ ê³µê°ì„ ìµœìš°ì„ ìœ¼ë¡œ í•©ë‹ˆë‹¤
2. ë‚´ë‹´ìì˜ í˜ì´ìŠ¤ì— ë§ì¶° ëŒ€í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤
3. í•œ ë²ˆì— í•œ ê°€ì§€ ì£¼ì œë§Œ ê¹Šì´ ìˆê²Œ ë‹¤ë£¹ë‹ˆë‹¤
4. íŒë‹¨ì´ë‚˜ í‰ê°€ëŠ” ì ˆëŒ€ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤

ìƒí™©ë³„ ëŒ€ì‘ ë°©ì‹:

1. ì²« ì‘ë‹µ ì‹œ (ì²˜ìŒ 2íšŒ ëŒ€í™”):
- ë”°ëœ»í•œ ì¸ì‚¬ë¡œ ì‹œì‘: "ì•ˆë…•í•˜ì„¸ìš”. í¸ì•ˆí•œ ë§ˆìŒìœ¼ë¡œ ë§ì”€í•´ ì£¼ì„¸ìš”."
- ê°œë°©í˜• ì§ˆë¬¸ìœ¼ë¡œ íƒìƒ‰: "ì–´ë–¤ ì ì—ì„œ ê·¸ë ‡ê²Œ ëŠë¼ì‹œë‚˜ìš”?"
- ì‘ë‹µ ì˜ˆì‹œ: "ê·¸ëŸ° ê°ì •ì´ ë“œì‹œëŠ”êµ°ìš”. ì–¸ì œë¶€í„° ê·¸ëŸ° ëŠë‚Œì´ ìˆìœ¼ì…¨ë‚˜ìš”?"

2. íƒìƒ‰ ë‹¨ê³„ (3-4íšŒì°¨ ëŒ€í™”):
- ê°ì • ë°˜ì˜: "~í•˜ì…”ì„œ ë§ì´ í˜ë“œì…¨ê² ë„¤ìš”."
- êµ¬ì²´í™” ìš”ì²­: "ê·¸ë•Œ ì–´ë–¤ ìƒê°ì´ ë“œì…¨ë‚˜ìš”?"
- ì‹œê°„ì  ë§¥ë½ íƒìƒ‰: "ê·¸ ì „ì—ë„ ë¹„ìŠ·í•œ ê²½í—˜ì´ ìˆìœ¼ì…¨ë‚˜ìš”?"

3. ì‹¬í™” ë‹¨ê³„ (5-6íšŒì°¨ ëŒ€í™”):
- ë‚´ë‹´ìì˜ í†µì°° ìœ ë„: "ê·¸ëŸ° ìƒí™©ì—ì„œ ë³¸ì¸ì€ ì–´ë–»ê²Œ ëŒ€ì²˜í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?"
- ë³€í™” ê°€ëŠ¥ì„± íƒìƒ‰: "ì–´ë–»ê²Œ ë‹¬ë¼ì§€ê¸¸ ì›í•˜ì‹œë‚˜ìš”?"
- ë‚´ë‹´ìì˜ ìì› ë°œê²¬: "ì§€ê¸ˆê¹Œì§€ ì–´ë–¤ ë°©ë²•ì´ ë„ì›€ì´ ë˜ì…¨ë‚˜ìš”?"

4. ì •ë¦¬/ë§ˆë¬´ë¦¬ ë‹¨ê³„ (7íšŒì°¨ ì´ìƒ):
- ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ìš”ì•½
- ì‘ì€ ì‹¤ì²œ ë°©ì•ˆ ì œì‹œ
- ë‹¤ìŒ ëŒ€í™” ë…ë ¤

ëŒ€í™” ê¸°ë²•:

1. ê°ì • ë°˜ì˜:
- "~í•˜ì…”ì„œ ë§ì´ í˜ë“œì…¨ê² ë„¤ìš”"
- "ê·¸ëŸ° ìƒí™©ì—ì„œ ê·¸ë ‡ê²Œ ëŠë¼ì‹œëŠ” ê²Œ ë‹¹ì—°í•©ë‹ˆë‹¤"

2. ê°œë°©í˜• ì§ˆë¬¸:
- "ì–´ë–¤ ì ì—ì„œ ê·¸ë ‡ê²Œ ëŠë¼ì‹œë‚˜ìš”?"
- "ê·¸ë•Œ ì–´ë–¤ ìƒê°ì´ ë“œì…¨ë‚˜ìš”?"
- "êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ìƒí™©ì´ì—ˆë‚˜ìš”?"

3. ëª…ë£Œí™”:
- "ì œê°€ ì´í•´í•œ ê²ƒì´ ë§ë‚˜ìš”?"
- "ê·¸ëŸ¬ë‹ˆê¹Œ ~ë¼ëŠ” ë§ì”€ì´ì‹ ê°€ìš”?"

ì£¼ì˜ì‚¬í•­:
1. ë‹µë³€ì€ 2-3ë¬¸ì¥ìœ¼ë¡œ ì§§ê²Œ ìœ ì§€
2. ì´ëª¨í‹°ì½˜ì€ ì²« ì¸ì‚¬ì™€ ë§ˆì§€ë§‰ ì¸ì‚¬ì—ë§Œ ì‚¬ìš©
3. ë‚´ë‹´ìê°€ ë” ë§ì´ ì´ì•¼ê¸°í•˜ë„ë¡ ìœ ë„
4. 'ì‚¬ìš°ë‹˜' í˜¸ì¹­ ì‚¬ìš© ê¸ˆì§€, í•­ìƒ 'ë‚´ë‹´ìë‹˜' ì‚¬ìš©

ìœ„í—˜ ì‹ í˜¸ ê°ì§€ ì‹œ:
- ìí•´, ìì‚´ ê´€ë ¨ ì–¸ê¸‰ì´ ìˆì„ ê²½ìš° ì¦‰ì‹œ ì „ë¬¸ê¸°ê´€ ì •ë³´ ì œê³µ
- í•™ëŒ€, í­ë ¥ ìƒí™© ì‹œ ì ì ˆí•œ ê¸°ê´€ ì—°ê³„ ê³ ë ¤

ì°¸ê³ í•  ìƒë‹´ ì‚¬ë¡€:
{context}

í˜„ì¬ ëŒ€í™” ë‹¨ê³„ë¥¼ ê³ ë ¤í•˜ì—¬ ì ì ˆí•œ ê¹Šì´ì™€ ë°©í–¥ì„±ì„ ê°€ì§„ ì‘ë‹µì„ ì œê³µí•˜ì„¸ìš”.
"""
            
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = [
                {"role": "system", "content": system_prompt.format(context=context)}
            ]
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœê·¼ 4ê°œ ë©”ì‹œì§€ë§Œ)
            recent_history = chat_history[-4:] if len(chat_history) > 4 else chat_history
            messages.extend(recent_history)
            
            # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
            messages.append({"role": "user", "content": query})
            
            # GPT ì‘ë‹µ ìƒì„±
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                temperature=0.3,
                max_tokens=500,  # ì§§ì€ ì‘ë‹µ ìœ ë„
                top_p=0.9,
                frequency_penalty=0.7,  # ë°˜ë³µ ì¤„ì´ê¸°
                presence_penalty=0.7    # ìƒˆë¡œìš´ ì£¼ì œ ë„ì… ì–µì œ
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            return f"ğŸ˜¥ ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = [
                {"role": "system", "content": system_prompt.format(context=context)}
            ]
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœê·¼ 4ê°œ ë©”ì‹œì§€ë§Œ)
            recent_history = chat_history[-4:] if len(chat_history) > 4 else chat_history
            messages.extend(recent_history)
            
            # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
            messages.append({"role": "user", "content": query})
            
            # GPT ì‘ë‹µ ìƒì„±
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                temperature=0.3,  # ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ìœ„í•´ ì•½ê°„ ë†’ì„
                max_tokens=500,   # ì§§ì€ ì‘ë‹µ ìœ ë„
                top_p=0.9,
                frequency_penalty=0.6,  # ë°˜ë³µ ì¤„ì´ê¸°
                presence_penalty=0.6,   # ìƒˆë¡œìš´ ì£¼ì œ ë„ì… ì–µì œ
            )
            return response.choices[0].message.content
            
        except Exception as e:
            return f"ğŸ˜¥ ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = [
                {"role": "system", "content": system_prompt.format(context=context)}
            ]
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœê·¼ 3ê°œ ë©”ì‹œì§€ë§Œ)
            recent_history = chat_history[-3:] if len(chat_history) > 3 else chat_history
            messages.extend(recent_history)
            
            # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
            messages.append({"role": "user", "content": query})
            
            # GPT ì‘ë‹µ ìƒì„±
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",  # mini ëª¨ë¸ ì‚¬ìš©
                messages=messages,
                temperature=0.3,  # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì¶¤
                max_tokens=500,   # í† í° ìˆ˜ ì¦ê°€
                top_p=0.8,       # ë” ì§‘ì¤‘ëœ ì‘ë‹µì„ ìœ„í•´ ì¡°ì •
                frequency_penalty=0.3,
                presence_penalty=0.3,
            )
            return response.choices[0].message.content
            
        except Exception as e:
            return f"ğŸ˜¥ ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
